
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="/theme/pygments/github.min.css">


  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/solid.css">





  


 

<meta name="author" content="Pierre Glaser" />
<meta name="description" content="I have been attending ICML virtually this year - a few papers caught my attention, mostly because they tackle issues related to what I am currently thinking about: generative models." />
<meta name="keywords" content="Generative Models">


  <meta property="og:site_name" content="Pierre Glaser"/>
  <meta property="og:title" content="Recap on ICML 2021"/>
  <meta property="og:description" content="I have been attending ICML virtually this year - a few papers caught my attention, mostly because they tackle issues related to what I am currently thinking about: generative models."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="/recap-on-icml-2021.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2021-07-28 19:00:00+01:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="/author/pierre-glaser.html">
  <meta property="article:section" content="Research"/>
  <meta property="article:tag" content="Generative Models"/>
  <meta property="og:image" content="/images/profile-picture.jpg">

  <title>Pierre Glaser &ndash; Recap on ICML 2021</title>

</head>
<body class="light-theme">
  <aside>
    <div>
      <a href="/">
        <img src="/images/profile-picture.jpg" alt="Pierre Glaser" title="Pierre Glaser">
      </a>

      <h1>
        <a href="/">Pierre Glaser</a>
      </h1>



      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="/pages/about.html#about">
                  About
                </a>
              </li>
              <li>
                <a target="_self"
                   href="/pages/research-interests.html#research-interests">
                  Research Interests
                </a>
              </li>
              <li>
                <a target="_self"
                   href="/pages/talks-and-presentations.html#talks-and-presentations">
                  Talks and Presentations
                </a>
              </li>

            <li>
              <a target="_self" href="/images/resume.pdf" >Resume</a>
            </li>
            <li>
              <a target="_self" href="/archives.html" >Archives</a>
            </li>
        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-github" href="https://github.com/pierreglaser" target="_blank">
              <i class="fab fa-github"></i>
            </a>
          </li>
          <li>
            <a  class="sc-twitter" href="https://twitter.com/PierreGlaser" target="_blank">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
          <li>
            <a  class="sc-linkedin" href="https://www.linkedin.com/in/pierre-glaser-b30587112/" target="_blank">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="recap-on-icml-2021">Recap on ICML 2021</h1>
    <p>
      Posted on Wed 28 July 2021 in <a href="/category/research.html">Research</a>

    </p>
  </header>


  <div>
    <p>I have been attending ICML virtually this year - a few papers caught my attention, mostly because they tackle issues related to what I am currently thinking about: generative models.</p>
<p>Summarizing a paper is a great exercise: putting thoughts into words requires to reach a substantial level of understanding of the paper. Some amount of litterature review is needed to understand the problem setting as well as previous work, which can help to form a high level mental image of the field. Moreover, unlike for your own papers, you can be as vocal as you want about potential weaknesses of the contributions, which help put things in perspective.</p>
<p>For that reason, I summarized two papers presented in ICML 2021: <em>Conjugate Energy Based Models</em>, and <em>Generative Particle Variational Inference via Estimation of Functional Gradients</em>. I focused on explaining the initial motivation and the high-level structure of the algorithm, and purposely skipped many details and training tricks described at length in the papers.</p>
<p>Finally, I also included a list of other papers that I enjoyed, but did not summarize.</p>
<h3><a href="http://proceedings.mlr.press/v139/wu21a.html">Conjugate Energy Based models (CEBM)</a></h3>
<p>TL;DR: CEBMs are Latent variables energy-based models that are trained by comparing structure in latent variable (<span class="math">\(z \in \mathbb R^K\)</span>) space instead of observed variable space (<span class="math">\(x \in \mathbb R^d, d \gg K\)</span>). Authors argue that comparing similarities in latent variable space is better as latent variables contain less variability coming from irrelevant noise.</p>
<p><img alt="Image" src="images/vae-vs-cebm.png">
<em>Differences between VAEs and CEBMs. Picture credits: CEBM paper.</em></p>
<h4>Background of Latent variable generative models</h4>
<p>This article introduces a new form of latent energy based models (LEBM). LEBMs are models of the form
</p>
<div class="math">$$p(x, z) = \frac{e^{-E_\theta(x, z)}}{Z(\theta)}$$</div>
<p>Fitting an LEBMs against some observed data <span class="math">\(\{x^{(i)}\}_{i=1}^{N}\)</span> is usually done using <a href="https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf">contrastive divergence</a>, which is a twist on approximate gradient-based Likelihood optimization with faster effective runtime. Energy based models are praised for their high flexibility: any structure on the observed and latent variables can be specified through the energy function <span class="math">\(E\)</span>.</p>
<h4>Structure of CEBMs</h4>
<p>Conditional Energy Based Models (introduced in the paper) define an energy <span class="math">\(E_\theta(x, z)\)</span> that ensures:</p>
<ul>
<li>A parametrized conditional <span class="math">\(p(x \mid z) \propto e^{\left \langle T_\theta(x), \eta(z) \right \rangle} (\eta(z) \in \mathbb R^H, H \sim K \ll d)\)</span> (with untractable normalizing constant). </li>
<li>A posterior <span class="math">\(p(z \mid x)\)</span> made tractable thanks to a conjugacy trick, allowing for cheap inference.</li>
<li>A parametrized prior-like bias on z, <span class="math">\(B_\lambda(z)\)</span> included in the energy <span class="math">\(E(x, z)\)</span>.</li>
</ul>
<p>The likelihood <span class="math">\(p(x)\)</span> of CEBM can be shown to be of the form <span class="math">\(e^{-E(x)}/Z\)</span>, with E known analytically. Due to the unknown normalizing constant <span class="math">\(Z\)</span>, (it is an EBM...), exact ML/Free-energy based learning is impossible, and as explained above, one must resort to training using contrastive divergence + MCMC (in their case using Stochastic Gradient Langevin Dynamics).</p>
<h4>VAEs vs CEBMs: structural differences in the Bregman Divergence sense</h4>
<p>One major alternative to LEBMs are Variational Auto-Encoders (VAEs). VAEs define a joint density <span class="math">\(p(x, z)\)</span> of the form
</p>
<div class="math">$$p(x, z) = p(x | z) p(z) = \mathcal N(\mu_\theta(z), \sigma^2 I) \mathcal N(0, 1)$$</div>
<p> that is analytically available, and thus can be trained using approximate free-energy maximization. VAEs, define an encoder network <span class="math">\(\mu_\theta(x)\)</span> that approximates the posterior <span class="math">\(p(z \mid x) = \mathcal N(\mu_\theta(x), \sigma^2 I)\)</span>.</p>
<p>Authors argue that VAEs compare structure in output variable space <span class="math">\(x\)</span>, while CEBMs compare structure in latent variable space <span class="math">\(z\)</span>. To justify that, authors rely on the duality characteristics of exponential families. Without entering in too much details, the log-likelihood <span class="math">\(\log p(x \mid \theta)\)</span> of an exponential family member with sufficient statistic <span class="math">\(t(x)\)</span>, natural parameter <span class="math">\(\eta\)</span>, (convex) log-normalizer <span class="math">\(A(\eta)\)</span> and mean parameter <span class="math">\(\mu\)</span> can be written as:
</p>
<div class="math">$$ \log p(x \mid \theta) = &lt;t(x), \eta&gt; - A(\eta) = -D_A(t(x), \mu) + A^\star(t(x))$$</div>
<p>
Where <span class="math">\(A^\star\)</span> is the fenchel conjugate of <span class="math">\(A\)</span>, and <span class="math">\(D_A\)</span> is the Bregman divergence associated to <span class="math">\(A\)</span>. Optimizing over <span class="math">\(\eta\)</span> is the same as optimizing over <span class="math">\(\mu\)</span>, and we see that maximizing the data likelihood is the same as minimizing the Bregman Divergence <span class="math">\(D_A\)</span> between <span class="math">\(t(x)\)</span> and <span class="math">\(\mu\)</span>.</p>
<p>Now, applying this to both VAEs and GEBMs, we get:</p>
<div class="math">$$ \log p_{\text{VAE}}(x, z) = D_A(x, \mu_\theta(z)) +E(x) + E_\lambda(z)$$</div>
<div class="math">$$ \log p_{\text{CEBM}}(x, z) = D_B(\eta(z), \mu_\theta(x)) - B^\star(\eta(z))  + E_{\theta, \lambda}(z)$$</div>
<p>Where <span class="math">\(B\)</span> is a tractable log-normalizer of the posterior <span class="math">\(p(z \mid x)\)</span>. Note that in the second line, <span class="math">\(\mu_\theta\)</span> is the mean parameter associated with the CEBM posterior, while in the first line <span class="math">\(\mu_\theta\)</span> is the natural parameter of the VAE conditional <span class="math">\(p(x | z)\)</span>. Arguably, the latent sufficient statistic (<span class="math">\(\eta(z)\)</span>) space <span class="math">\(\mathbb R^H\)</span>, on which <span class="math">\(D_B\)</span> is defined is of similar complexity than the latent variable space <span class="math">\(\mathbb R^H\)</span>, and it is fair to say that CEBMs carry out comparison in latent space, wherease VAEs do it in output space.</p>
<h3><a href="http://proceedings.mlr.press/v139/ratzlaff21a.html">Generative Particle Variational Inference via Estimation of Functional Gradients</a></h3>
<h4>Background on particle variational inference</h4>
<p>Particle Variational Inference (ParVI) methods are methods which propose to solve the problem of drawing samples from target probability measure with density <span class="math">\(p(x)\)</span> known up to a normalization factor. The solution of ParVI methods is to transport a set of particles <span class="math">\(\{x^{(i)}_0\}_{i=1}^{N}\)</span> (usually i.i.d, samples from a fixed density <span class="math">\(q_0\)</span>) "towards" regions of target high density. As I see it, the rationale for it is that ParVI methods can be seen as generating a path <span class="math">\(t \longmapsto \sum_{i=1}^{N} \delta_{x^{(i)}_t}\)</span> that should approximate a gradient flow path <span class="math">\(D(\cdot \mid \mid p(x))\)</span> starting from <span class="math">\(q_0\)</span>. Typically, D is the KL divergence, as proposed in the famous Stein Variational Gradient Descent paper.</p>
<p>The main limitation of ParVI methods is that the number of samples N is fixed upfront: there is no way to sample from our approximated target, whose existence we assume when the algorithm converges. Thus, authors in this paper propose to learn an implicit generative model <span class="math">\(q_\eta = (f_\eta)_{\#} q_0\)</span></p>
<h4>Generative particle VI: previous work</h4>
<p><a href="https://arxiv.org/pdf/1611.01722.pdf">Liu &amp; Wang (2016)</a> proposed to make particle VI methods generative. In their paper, there is not a deterministic set of paritcles anymore: these particles are replaced with an implicit generative model <span class="math">\((f_\eta)_\# p_0\)</span> trained as follows:</p>
<p>At each time <span class="math">\(t\)</span>:</p>
<ol>
<li>draw <span class="math">\(\{\xi_t^{(i)}\}_{i=1}^{N}\)</span> from <span class="math">\(p_0\)</span>, comute <span class="math">\(x_t^{(i)} = f_\eta(\xi_t^{(i)})\)</span> for all <span class="math">\(i\)</span>.</li>
<li>compute SVGD descent function <span class="math">\(\phi^\star\)</span>, update <span class="math">\(x_{t + 1} := x_t^{(i)} - \epsilon \phi^\star(x_t^{(i)})\)</span></li>
<li>update <span class="math">\(\eta\)</span> to map <span class="math">\(\xi_t^{{(i)}}\)</span> to <span class="math">\(x_{t+1}^{(i)}\)</span></li>
</ol>
<p><img alt="Image" src="images/parvi-methods.png">
<em>Particle VI methods draw samples from a target density <span class="math">\(p(x)\)</span> (gray level
lines) by transporting initial samples , usually using gradient flow dynamics.
Pictured: Stein Variational Gradient Descent trajectories, and LAWGD
trajectories (picture credits: <a href="https://arxiv.org/pdf/2006.02509.pdf">SVGD as a gradient flow of the Chi-Squared divergence</a>).</em></p>
<h4>Contributions</h4>
<p>Authors of the GPVI paper argue that the algorithm above has a flaw: it still uses a <strong>nonparametric</strong> descent direction <span class="math">\(\phi^\star\)</span>, while it should take into acount the generative model parametrization <span class="math">\(x = f_\eta(z)\)</span>, which implies that not all perturbations <span class="math">\((id + \epsilon f)_\# p_t\)</span> can be spanned by the model.  Additional tricks are then ensured to make the computations tractable.</p>
<p>One of the main argument of why this method is "principled" is the use of functional gradient (e.g. a gradient of a functional defined in a hilbert space <span class="math">\(\mathcal H\)</span> of functions, set to be a RKHS in the paper) to compute the "steepest descent" direction of <span class="math">\(\mathcal J(f_\eta) = \text{KL}((f_\eta)_\#q_0 \mid \mid p)\)</span>. The update on the parameters <span class="math">\(\eta\)</span> relies on this steepest direction in the following way:</p>
<ol>
<li>Compute a nonparametric functional gradient <span class="math">\(\nabla_f J(f_\eta) := \nabla (f \longmapsto \mathcal J(f))|_{f = f\eta}\)</span></li>
<li>"Backpropagate" the gradient to parameter space using the formula: <span class="math">\(\nabla_\eta \mathcal J(f_\eta) := \mathbb E_z \left \lbrack \frac{\partial f_\eta(z)}{\partial \eta}\nabla_f \mathcal J(f_\eta)(z) \right \rbrack\)</span></li>
</ol>
<p>However, I could not figure how they got the update rule for the parameter, which does not seem to be a simple application of the chain rule. In addition, the use of an RKHS when computing <span class="math">\(\nabla_f J\)</span> seems to imply that <span class="math">\(\{f_\eta \mid \eta \in \mathbb R^d\} = \mathcal H\)</span> which is not the case.</p>
<h3>Other papers that I liked</h3>
<ul>
<li><a href="http://proceedings.mlr.press/v139/grathwohl21a.html">Oops I Took A Gradient: Scalable Sampling for Discrete Distributions</a>: great delivery by Grathwohl, introducing a more efficient alternative to Gibbs Sampling with examples on Ising models.</li>
<li><a href="https://icml.cc/virtual/2021/poster/10275TLDR">Two-way kernel matrix puncturing: towards resource-efficient PCA and spectral clustering</a>. TLDR: you don't need your whole gram matrix when doing spectral clustring (in the M.O.G case). Continuation of a line of work from Romain, see <a href="https://arxiv.org/pdf/2001.08370.pdf">Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures</a>, and see <a href="https://arxiv.org/pdf/1510.03547.pdf">kernel  spectral  clustering  of  large  dimensional  data</a> for an analysis of a wider class of kernel matrices.</li>
<li><a href="https://icml.cc/virtual/2021/poster/9537">Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition</a>. Because kernels, and because it is the first time I heard of this harmonic kernel decomposition, a.k.a some form of Fourier-based orthogonal direct sum decomposition of a RKHS.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/generative-models.html">Generative Models</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Pierre Glaser ",
  "url" : "",
  "image": "/images/profile-picture.jpg",
  "description": ""
}
</script>

</body>
</html>