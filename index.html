<!DOCTYPE html>
<html>
  <head>
    <link href="css/style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript", src="assets/theme.js"></script>
    <title>Pierre Glaser</title>
  </head>
  <body>

  <div class="nav">
  <ul>
    <li><a href="/index.html">Home</a></li>
    <li><a href="/about.html">About</a></li>
    <li><a href="/blog.html">Blog</a></li>
    <li><a href="/talks.html">Talks (Scientific Computing)</a></li>
    <li style="float:right;">
      <button id="theme-toggle" onclick=toggleTheme() style="border:none;cursor:pointer;font-size:x-large;float:right;">
        <i id="img-theme-toggle" class="fa-regular fa-moon"></i>
      </button>
    </li>
  </ul>
  </div>

    <br>
    <br>

    <h1 style="padding-bottom:0.5cm;">
      Pierre Glaser
    </h1>
    <div class="social-media-links">
      <a href="https://github.com/pierreglaser" target="_blank" style="text-decoration:none;">
        <i class="fa fab fa-github faperso"></i>
      </a>
      <a href="https://twitter.com/pierreglaser" target="_blank" style="text-decoration:none">
        <i class="fa fab fa-twitter faperso"></i>
      </a>
      <a href="https://www.linkedin.com/in/pierre-glaser-b30587112" target="_blank" style="text-decoration:none">
        <i class="fab fa-linkedin-in faperso"></i>
      </a>
      <a href="https://scholar.google.com/citations?user=Yphu0qMAAAAJ&hl=en&oi=ao" target="_blank" style="text-decoration:none">
        <svg height="22px" width="22px" viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg" style="filter:var(--filter);"><path class="google-scholar" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952 0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584 0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288 0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962 0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zM1658.858 1512.573c-64.358 64.424-141.86 96.57-232.572 96.57h-1097.142c-90.712 0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572v-1097.142c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712 0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159v-392.126c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162 0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53 0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476 0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86 0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908 0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426 0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432 0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234 0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048 0 6.642 0.19 12.492 0.672 18.974h-261.046l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382 0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994 0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376 0 103.050 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382 0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.050 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z" /></svg>
      </a>
  </div>

  <div class="intro" style="padding-top:1cm">
    <p>
      I am a final-year Machine Learning PhD Student in the Gatsby Computational Neuroscience Unit. More about me <a href="/about.html">here</a>.
    </p>
  </div>

  <br>
  <br>

  <h2 style="padding-bottom:0.3cm;">
    Recent Publications
  </h2>
  <br>

  <div class="blog-card">
    <div class="content">
        <div style="font-weight:normal;font-size:16px;padding-bottom:10px;"> Under Review (2024) <a target="_blank" href="https://www.biorxiv.org/content/10.1101/2024.11.11.623030v1">(bioRxiv)</a></div>
        <h2 class="title">SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour</h2>
      <p class="summary">
      High-dimensional neural activity in the brain is known to encode low-dimensional, time-evolving, behaviour-related variables. A fundamental goal of neural data analysis consists of identifying such variables and their mapping to neural activity. The
      canonical approach is to assume  <span class="dots">...</span><span class="more hidden">
the latent variables are behaviour and visualize the subsequent tuning curves. However,      
significant mismatches between behaviour and the encoded variables may still exist — the agent may be thinking of another location, or be uncertain of its own — distorting the tuning curves 
and decreasing their interpretability. To address this issue a variety of methods have been proposed to learn this latent variable in an unsupervised manner; these techniques are typically expensive to train, come with many hyperparameters or scale poorly to large datasets complicating their adoption in practice. To solve these issues we propose SIMPL (Scalable Iterative Maximization of Population-coded Latents), an EM-style algorithm which iteratively optimizes latent variables and tuning curves. SIMPL is fast, scalable and exploits behaviour as an initial condition to further improve convergence and identifiability. We show SIMPL accurately recovers latent variables in biologically-inspired spatial and non-spatial tasks. When applied to a large rodent hippocampal dataset SIMPL efficiently finds a modified latent space with smaller, more numerous, and more uniformly-sized place fields than those based on behaviour, suggesting the brain may encode space with greater resolution than previously thought.
      </span></p>
    </div>
<button class="toggle-btn" type="button">Read more</button>
  </div>
  <br>
  <br>
  <div class="blog-card">
    <div class="content">
        <div class="meta"> NeurIPS 2024 <a target="_blank" href="https://openreview.net/pdf?id=Q74JVgKCP6">(pdf)</a></div>
        <h2 class="title">Near-Optimality of Contrastive Divergence Algorithms</h2>
      <p class="summary">
      We provide a non-asymptotic analysis of the contrastive divergence (CD) algorithm, a training method for unnormalized models. While prior work has established that (for exponential family distributions) the CD iterates asymptotically converge at an \(O(n^{-1/3})\) rate to the true parameter of the data distribution      <span class="dots">...</span><span class="more hidden">
, we show that CD can achieve the parametric rate . Our analysis provides results for various data batching schemes, including fully online and minibatch. We additionally show that CD is near-optimal, in the sense that its asymptotic variance is close to the Cramér-Rao lower bound.
      </span></p>
    </div>
<button class="toggle-btn" type="button">Read more</button>
  </div>
  <br>
  <br>
  <div class="blog-card">
    <div class="content">
        <div class="meta"> Under Review (2024) <a target="_blank" href="https://arxiv.org/abs/2409.14980">(ArXiv)</a></div>
        <h2 class="title">(De)-regularized Maximum Mean Discrepancy Gradient Flow</h2>
      <p class="summary">
      We introduce a (de)-regularization of the Maximum Mean Discrepancy (DrMMD) and its Wasserstein gradient flow. Existing gradient flows that transport samples from source distribution to target distribution with only target samples, either lack tractable numerical implementations
      <span class="dots" style>...</span><span class="more hidden">
(f-divergence flows) or require strong assumptions, and modifications such as noise injection, to ensure convergence (Maximum Mean Discrepancy flows). In contrast, DrMMD flow can simultaneously (i) guarantee near-global convergence for a broad class of targets in both continuous and discrete time, and (ii) be implemented in closed form using only samples. The former is achieved by leveraging the connection between the DrMMD and the χ2-divergence, while the latter comes by treating DrMMD as MMD with a de-regularized kernel. Our numerical scheme uses an adaptive de-regularization schedule throughout the flow to optimally trade off between discretization errors and deviations from the χ2 regime. The potential application of the DrMMD flow is demonstrated across several numerical experiments, including a large-scale setting of training student/teacher networks.
      </span></p>
    </div>
<button class="toggle-btn" type="button">Read more</button>
  </div>
  <br>
  <br>
  <div class="blog-card">
    <div class="content">
        <div class="meta"> ICML 2024 <a target="_blank" href="https://proceedings.mlr.press/v235/glaser24a.html">(pdf)</a></div>
        <h2 class="title">Kernel-Based Evaluation of Conditional Biological Sequence Models</h2>
      <p class="summary">
      We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy 
      <span class="dots">...</span><span class="more hidden">
        between the true conditional distribution and the model’s estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model’s temperature hyperparameter to achieve a better fit.
      </span></p>
    </div>
<button class="toggle-btn" type="button">Read more</button>
  </div>
  <br>
  <br>
  <div class="blog-card">
    <div class="content">
        <div class="meta"> UAI 2023, <span style="text-decoration:underline;font-weight:bold">Spotlight presentation</span> <a target="_blank" href="https://proceedings.mlr.press/v216/glaser23a.html">(pdf)</a></div>
        <h2 class="title">Fast and Scalable Score-Based Kernel Calibration Tests</h2>
      <p class="summary">
      We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a nonparametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods,
      <span class="dots">...</span><span class="more hidden"> test avoids the need for possibly expensive expectation approximations while providing control over its type-I error.
        <br>
        <br>

        We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a Conditional Goodness of Fit criterion for the KCCSD test's U-statistic.
        <br>
        <br>

        The tractability of the KCCSD test widens the surface area of calibration measures to new promising use-cases, such as regularization during model training. We demonstrate the properties of our test on various synthetic settings.
      </span></p>
    </div>
<button class="toggle-btn"  type="button">Read more</button>
  </div>
  <br>
  <br>

  <div class="blog-card">
    <div class="content">
        <div class="meta"> October 2022 - Under Review <a target="_blank" href="https://arxiv.org/abs/2210.14756">(ArXiv)</a></div>
        <h2 class="title">Learning Unnormalized Models for Simulation-Based Inference</h2>
      <p class="summary">
      We introduce two synthetic likelihood methods for Simulation-Based Inference (SBI), to conduct either amortized or targeted inference from experimental observations when a high-fidelity simulator is available.
Both methods learn a conditional<span class="dots">...</span><span class="more hidden">
energy-based model (EBM) of the likelihood using synthetic data generated by the
simulator, conditioned on parameters drawn from a proposal distribution.
The learned likelihood can then be combined with any prior to obtain a posterior estimate,
from  which samples can be drawn using MCMC.
<br>
<br>
Our methods uniquely combine a flexible Energy-Based Model and the minimization of a KL loss: this is in contrast to other synthetic likelihood methods, which either rely on normalizing flows, or minimize score-based objectives; choices that come with known pitfalls.
<br>
<br>
We demonstrate the properties of both methods on a range of synthetic datasets, and apply them to a neuroscience model of the pyloric network in the crab, where our method outperforms prior art for a fraction of the simulation budget.
      </span></p>
    </div>
<button class="toggle-btn" type="button">Read more</button>
  </div>
  <br>
  <br>

  <div class="blog-card">
    <div class="content">
        <div class="meta"> October 2021 - Accepted at NeurIPS 2021 <a target="_blank" href="https://arxiv.org/abs/2106.08929"> (ArXiv) </a> </div>
        <h2 class="title">KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support</h2>
      <p class="summary">
      We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergence between a moving source and a fixed target distribution. This approximation, termed the KALE (KL approximate lower-bound estimator)<span class="dots">...</span><span class="more hidden">

      solves a regularized version of the Fenchel dual problem defining the KL over a restricted class of functions.
      <br>
      <br>
      When using a Reproducing Kernel Hilbert Space (RKHS) to define the function class, we show that the KALE continuously interpolates between the KL and the Maximum Mean Discrepancy (MMD). Like the MMD and other Integral Probability Metrics, the KALE remains well defined for mutually singular distributions. Nonetheless, the KALE inherits from the limiting KL a greater sensitivity to mismatch in the support of the distributions, compared with the MMD.
      <br>
      <br>
      These two properties make the KALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE's properties.
      </span></p>
<button class="toggle-btn" type="button">Read more</button>
    </div>
  </div>

  <!-- news section-->
  <div class="news" style="padding-top:1cm">
    <h2 style="padding-bottom:0.3cm;">
      News
    </h2>
    <ul style="list-style: disc;">
      <li style="line-height: 22pt;">
        <p><b>2024-12-01</b>: I received a top reviewer award at NeurIPS 2024!</p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2024-11-11</b>: New <a target="_blank" href=https://openreview.net/pdf?id=Q74JVgKCP6https://www.biorxiv.org/content/10.1101/2024.11.11.623030v1>preprint</a>, where we construct a new simple and interpretable neural data analyis method.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2024-09-30</b>: New <a target="_blank" href=https://openreview.net/pdf?id=Q74JVgKCP6>paper</a> on the near-optimality of Contrastive Divergence Algorithms.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2024-06-01</b>: New <a target="_blank" href=https://arxiv.org/abs/2409.14980>preprint</a> on constructing MMD gradient flows with stronger convergence guarantees.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2024-06-01</b>: New <a target="_blank" href=https://proceedings.mlr.press/v235/glaser24a.html>paper</a> on evalauting Conditional Biological Sequence Models (a.k.a Protein Design Models).
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2023-10-01</b>: I just started an internship at the <a href=http://probcomp.csail.mit.edu/> MIT Probabilistic Computing Project </a>!
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2023-02-20</b>: New <a href=https://proceedings.mlr.press/v216/glaser23a.html>paper</a> on testing for the calibration of Probabilistic Models!
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p><b>2023-01-03</b>: I was nominated among the top 10% of reviewers of AISTATS 2023!</p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2022-10-30</b>: I gave a  <a href="talks/pyclub-jax/reveal.js/index.html" target="_blank">tutorial on JAX</a> at the UCL Advanced Research Computing Centre.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2022-09-28</b>: New <a href="https://arxiv.org/abs/2210.14756" target="_blank"> preprint </a> on Learning Unnormalized Models for Simulation-Based Inference.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2021-10-01</b>: My <a href="https://arxiv.org/abs/2106.08929" target="_blank"> work </a> on Relaxing the KL Gradient Flow was accepted at NeurIPS 2021!
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2020-08-30</b>: I just started my PhD at the <a href="https://www.ucl.ac.uk/gatsby/gatsby-computational-neuroscience-unit"> Gatsby Computational Neuroscience Unit</a>.
        </p>
      </li>
      <li style="line-height: 22pt;">
        <p>
        <b>2020-08-30</b>: I Graduated from the <a href="https://www.master-mva.com/"> MVA Master </a> in Machine Learning.
        </p>
      </li>
    </ul>
  </div>
  <script src="assets/readmore.js"></script>
  </body>
</html>
<!DOCTYPE html>
<!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
